# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vbe2svvqnYHB-xl-v8VcD9oG38vqwCww
"""


import tvm
from tvm import relay
from tvm.contrib import graph_executor
import os
from keras.datasets import mnist # download mnist data and split into train and test sets
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Flatten
import numpy as np
import matplotlib.pyplot as plt # plot the first image in the dataset
import keras

print(tvm.__version__)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'

(X1_train, y_train), (X1_test, y_test) = mnist.load_data()

print("Image shape: ", X1_train[0].shape)


# reshape data to fit model
X_train = X1_train.reshape(60000,28,28,1)
X_test = X1_test.reshape(10000,28,28,1)



y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

print(y_train[0])

# # create model
# model = Sequential()
# #add model layers
# model.add(Conv2D(64, kernel_size=3, activation="relu", input_shape=(28,28,1), name="input"))
# model.add(Conv2D(32, kernel_size=3, activation="relu"))
# model.add(Flatten())
# model.add(Dense(10, activation="softmax"))
#
# model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
# model.summary()
#
# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)

# model.save("my_model.h5")
#///////////////////////////////////////////////////////////////////////////////////////////////start tvm
model = keras.models.load_model("my_model.h5")

# predict first 4 images in the test set
print("Prediction: ", np.argmax(model.predict(X_test[:10]), axis=1))
print("Labels:     ", np.argmax(y_test[:10], axis=1))

input_shape = [1, 28, 28, 1] # [batch, height, width, channels]
shape_dict = {"input_input": input_shape}
mod, params = relay.frontend.from_keras(model, shape_dict, layout="NHWC")

print(mod)

target = tvm.target.Target("llvm  -mcpu=core-avx2")
dev = tvm.cpu(0)

with tvm.transform.PassContext(opt_level=3): #проводим тесты над нейросетью в tvm //    tvm_model(data.reshape(1, 28, 28, 1)).numpy()[0]
    tvm_model = relay.build_module.create_executor("graph", mod, dev, target, params).evaluate()

print("||||||||||||||||||||||||")
print(tvm_model)

out = []
for data in X_test[:10]:
    # HWC -> NHWC
    out.append(tvm_model(data.reshape(1, 28, 28, 1)).numpy()[0]) #проводим тесты над нейросетью в tvm //    tvm_model(data.reshape(1, 28, 28, 1)).numpy()[0]

print("Prediction: ", np.argmax(out, axis=1))
print("Labels:     ", np.argmax(y_test[:10], axis=1))



# print(out)
#
# print(y_test[:10])
#///////////////////////////////////////////////////////////////////////////////////////////////////////////////start tunning
def evaluate_performance(lib, data_shape, dtype="float32"):
    # upload parameters to device
    dev = tvm.cpu()
    data_tvm = tvm.nd.array((np.random.uniform(size=data_shape)).astype(dtype))
    module = graph_executor.GraphModule(lib["default"](dev))
    module.set_input(input_name, data_tvm)

    # evaluate
    print("Evaluate inference time cost...")
    print(module.benchmark(dev, number=100, repeat=3))
##


model_name = "my_mnist_model"
input_name = "input_input"
target = tvm.target.Target("llvm -mcpu=core-avx2")

import multiprocessing
# Set number of threads used for tuning based on the number of
# physical CPU cores on your machine.
num_threads = multiprocessing.cpu_count()
print("Num threads: ", int(num_threads))
os.environ["TVM_NUM_THREADS"] = str(int(num_threads))

from tvm import auto_scheduler
log_file = "my_mnist_model.auto-scheduler.log"
# log_file = "%s.auto-scheduler.log" % model_name
#
# # extract workloads from relay program
# def extract_tasks(mod, target, params):
#     print("Mod:")
#     print(mod)
#     print("Extract tasks...")
#     tasks, task_weights = auto_scheduler.extract_tasks(mod, params, target)
#     assert(len(tasks) > 0)
#
#
#     for idx, task in enumerate(tasks):
#         print("Task: %d, desc: %s" % (idx, task.desc))
#     return tasks, task_weights
#
# tasks, task_weights = extract_tasks(mod, target, params)
# print (tasks)
# print("|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||")
# #print (task_weights)
#
# def run_tuning(tasks, task_weights, log_file, n_trials):
#     print("Begin tuning...")
#     tuner = auto_scheduler.TaskScheduler(tasks, task_weights)
#     tune_option = auto_scheduler.TuningOptions(
#         num_measure_trials=n_trials,  # change this to 20000 to achieve the best performance
#         runner=auto_scheduler.LocalRunner(repeat=10, enable_cpu_cache_flush=True),
#         measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
#     )
#
#     tuner.tune(tune_option)
#
# trials = len(tasks) * 2 * 64
# print (trials)
# run_tuning(tasks, task_weights, log_file,  trials)

def evaluate(module, data_shape, log_file, target="llvm"):
    # compile kernels in default mode
    print("Evaluation of the network compiled in 'default' mode without auto tune:")
    with tvm.transform.PassContext(opt_level=3):
        print("Compile...")
        lib = relay.build(module, target=target, params=params)
        evaluate_performance(lib, data_shape)

    # compile kernels in kernel tuned only mode
    print("\nEvaluation of the network been tuned on kernel level:")
    with auto_scheduler.ApplyHistoryBest(log_file):
        print("Compile...")
        with tvm.transform.PassContext(opt_level=3, config={"relay.backend.use_auto_scheduler": True}):
            lib = relay.build(module, target=target, params=params)
        evaluate_performance(lib, data_shape)


evaluate(mod, input_shape, log_file, target)

print("||||||||||||||||||||||||||||||||||||||||||run on opt neural network||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||")
with auto_scheduler.ApplyHistoryBest(log_file):
    with tvm.transform.PassContext(opt_level=3, config={"relay.backend.use_auto_scheduler": True}):
        lib = relay.build(mod, target=target, params=params)

module = graph_executor.GraphModule(lib["default"](dev))
# save(file_name, fmt='')¶
out = []
for data in X_test[:30]:
    module.set_input(input_name, data.reshape(1, 28, 28, 1))
    module.run()
    numOutputs = module.get_num_outputs()

    out.append(module.get_output(0).numpy()[0])
print(np.argmax(out, axis=1) == np.argmax(y_test[:30], axis=1))
print("Prediction: ", np.argmax(out, axis=1))
print("Labels:     ", np.argmax(y_test[:30], axis=1))
print("|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||")
#for data in X_test[:10]:
    # HWC -> NHWC
    #out.append(tvm_model(data.reshape(1, 28, 28, 1)).numpy()[0])

# X1_test = np.reshape(X1_test, input_shape)
# # Преобразование данных списка X1_test в тип float32
# X1_test = np.asarray(X1_test, dtype=np.float32)

# Создание массива data_tvm с использованием данных из X1_test
#dtype="float32"

# for X1 in X1_test:
#     X1_resh = X1.reshape(1,28,28,1)
#     data_tvm =  tvm.nd.array(X1_resh)
#     dev = tvm.cpu()
#     #data_tvm = tvm.nd.array((np.random.uniform(size=data_shape)).astype(dtype))
#     module = graph_executor.GraphModule(lib["default"](dev))
#     module.set_input(input_name, data_tvm)
#
#
# print("Prediction: ", np.argmax(out, axis=1))
# print("Labels:     ", np.argmax(y_test[:10], axis=1))

print("||||||||||||||||||||||||||||||||послойная статистика||||||||||||||||||||||||||||||||||||||||||")
from tvm.contrib.debugger import debug_executor

def collect_per_layer_stat(lib, device, json_graph=None):
    if json_graph is None:
        json_graph = lib.get_graph_json()
    debug_module = debug_executor.GraphModuleDebug(lib["debug_create"]("default", device), [device], json_graph, None)
    debug_module.run(number=10, repeat=3)

print("auto_scheduler")

collect_per_layer_stat(lib,dev)

print("default")
with tvm.transform.PassContext(opt_level=3):
    print("Compile...")
    libDefault = relay.build(mod, target=target, params=params)

collect_per_layer_stat(libDefault,dev)

print("||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||")

