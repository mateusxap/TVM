2024-04-30 19:06:05 [INFO] [task_scheduler.cc:160] Initializing Task #9: "fused_nn_conv2d_add_multiply_add_nn_relu"
2024-04-30 19:06:05 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)))
        conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(38), T.int64(38), T.int64(3)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, yy, xx, ff, ry, rx, rc in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64), T.int64(7), T.int64(7), T.int64(3)):
            with T.block("conv2d_nhwc"):
                v_nn, v_yy, v_xx, v_ff, v_ry, v_rx, v_rc = T.axis.remap("SSSSRRR", [nn, yy, xx, ff, ry, rx, rc])
                T.reads(pad_temp[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                with T.init():
                    conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + pad_temp[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T_add[v_ax0, v_ax1, v_ax2, v_ax3] * p3[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-04-30 19:06:05 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-30 19:06:05 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 0, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0, nn_1, yy_1, xx_1, ff_1, ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(16), T.int64(1), T.int64(1), T.int64(7), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                with T.block("conv2d_nhwc"):
                    v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                    v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(4) + yy_1 * T.int64(4) + yy_2 + yy_3)
                    v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(16) + xx_1 * T.int64(16) + xx_2 + xx_3)
                    v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(4) + ff_2 * T.int64(4) + ff_3)
                    v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                    v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                    v_rc = T.axis.reduce(T.int64(3), rc_0 * T.int64(3) + rc_1)
                    T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                    T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                    T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                    with T.init():
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                    conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
            for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
                with T.block("T_relu"):
                    v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                    T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 4])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v57 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v57)
2024-04-30 19:06:05 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0, nn_1, yy_1, xx_1, ff_1 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16)):
                for ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(16), T.int64(1), T.int64(1), T.int64(7), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("conv2d_nhwc"):
                        v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                        v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(4) + yy_1 * T.int64(4) + yy_2 + yy_3)
                        v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(16) + xx_1 * T.int64(16) + xx_2 + xx_3)
                        v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(4) + ff_2 * T.int64(4) + ff_3)
                        v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                        v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                        v_rc = T.axis.reduce(T.int64(3), rc_0 * T.int64(3) + rc_1)
                        T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                        T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                        T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                        with T.init():
                            conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(16), T.int64(4)):
                    with T.block("T_relu"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(16), yy_0 * T.int64(4) + ax1)
                        v_ax2 = T.axis.spatial(T.int64(16), ax2)
                        v_ax3 = T.axis.spatial(T.int64(64), ff_1 * T.int64(4) + ax3)
                        T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 4])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
b57, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b57, loop=l42, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
2024-04-30 19:06:05 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                for nn_1, yy_1, xx_1, ff_1, ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(7), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(16), T.int64(1), T.int64(1), T.int64(7), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                    with T.block("conv2d_nhwc"):
                        v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                        v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(4) + yy_1 * T.int64(4) + yy_2 + yy_3)
                        v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(16) + xx_1 * T.int64(16) + xx_2 + xx_3)
                        v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(4) + ff_2 * T.int64(4) + ff_3)
                        v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                        v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                        v_rc = T.axis.reduce(T.int64(3), rc_0 * T.int64(3) + rc_1)
                        T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                        T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                        T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                        with T.init():
                            conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(16), T.int64(64)):
                    with T.block("T_relu"):
                        v_ax0 = T.axis.spatial(T.int64(1), ax0)
                        v_ax1 = T.axis.spatial(T.int64(16), yy_0 * T.int64(4) + ax1)
                        v_ax2, v_ax3 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 4, 1])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 16, 1])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 16, 1, 4])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
b57, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b57, loop=l41, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
2024-04-30 19:13:53 [INFO] [task_scheduler.cc:160] Initializing Task #9: "fused_nn_conv2d_add_multiply_add_nn_relu"
2024-04-30 19:13:53 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)))
        conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_add = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_multiply = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        T_add_1 = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(38), T.int64(38), T.int64(3)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(p0[v_i0, v_i1, v_i2, v_i3])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = p0[v_i0, v_i1, v_i2, v_i3]
        for nn, yy, xx, ff, ry, rx, rc in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64), T.int64(7), T.int64(7), T.int64(3)):
            with T.block("conv2d_nhwc"):
                v_nn, v_yy, v_xx, v_ff, v_ry, v_rx, v_rc = T.axis.remap("SSSSRRR", [nn, yy, xx, ff, ry, rx, rc])
                T.reads(pad_temp[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                with T.init():
                    conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + pad_temp[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_add[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add[v_ax0, v_ax1, v_ax2, v_ax3] = conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_multiply"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add[v_ax0, v_ax1, v_ax2, v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3])
                T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] = T_add[v_ax0, v_ax1, v_ax2, v_ax3] * p3[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_add_1"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_multiply[v_ax0, v_ax1, v_ax2, v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                T.writes(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T_add_1[v_ax0, v_ax1, v_ax2, v_ax3] = T_multiply[v_ax0, v_ax1, v_ax2, v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
            with T.block("T_relu"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3])
                T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max(T_add_1[v_ax0, v_ax1, v_ax2, v_ax3], T.float32(0))
2024-04-30 19:13:53 [INFO] [task_scheduler.cc:164] Total 3 design space(s) generated
2024-04-30 19:13:53 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 64, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0, nn_1, yy_1, xx_1, ff_1, ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(16), T.int64(4), T.int64(2)):
                with T.block("conv2d_nhwc"):
                    v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                    v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(16) + yy_1 * T.int64(16) + yy_2 * T.int64(16) + yy_3)
                    v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(8) + xx_1 * T.int64(8) + xx_2 * T.int64(4) + xx_3)
                    v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(8) + ff_2 * T.int64(2) + ff_3)
                    v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                    v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                    v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1)
                    T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                    T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                    T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                    with T.init():
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                    conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
            for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(64)):
                with T.block("T_relu"):
                    v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                    T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                    T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                    T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 16])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v57 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v57)
2024-04-30 19:13:53 [INFO] [task_scheduler.cc:170] Design space #1:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 512, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0, nn_1, yy_1, xx_1, ff_1 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8)):
                for ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(7), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(16), T.int64(4), T.int64(2)):
                    with T.block("conv2d_nhwc"):
                        v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                        v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(16) + yy_1 * T.int64(16) + yy_2 * T.int64(16) + yy_3)
                        v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(8) + xx_1 * T.int64(8) + xx_2 * T.int64(4) + xx_3)
                        v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(8) + ff_2 * T.int64(2) + ff_3)
                        v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                        v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                        v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1)
                        T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                        T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                        T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                        with T.init():
                            conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(8), T.int64(8)):
                    with T.block("T_relu"):
                        v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                        v_ax2 = T.axis.spatial(T.int64(16), xx_0 * T.int64(8) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(64), ff_1 * T.int64(8) + ax3)
                        T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 16])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
b57, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b57, loop=l42, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
2024-04-30 19:13:54 [INFO] [task_scheduler.cc:170] Design space #2:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer((T.int64(1), T.int64(38), T.int64(38), T.int64(3)), "float32"), p1: T.Buffer((T.int64(7), T.int64(7), T.int64(3), T.int64(64)), "float32"), p2: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p3: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), p4: T.Buffer((T.int64(1), T.int64(1), T.int64(1), T.int64(64)), "float32"), T_relu: T.Buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)), "float32")):
        T.func_attr({"layout_free_buffers": [1], "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel": 96, "meta_schedule.unroll_explicit": 0, "meta_schedule.vectorize": 64})
            conv2d_nhwc = T.alloc_buffer((T.int64(1), T.int64(16), T.int64(16), T.int64(64)))
            for nn_0, yy_0, xx_0, ff_0 in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                for nn_1, yy_1, xx_1, ff_1, ry_0, rx_0, rc_0, nn_2, yy_2, xx_2, ff_2, ry_1, rx_1, rc_1, nn_3, yy_3, xx_3, ff_3 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(7), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(7), T.int64(1), T.int64(1), T.int64(16), T.int64(4), T.int64(2)):
                    with T.block("conv2d_nhwc"):
                        v_nn = T.axis.spatial(T.int64(1), nn_0 + nn_1 + nn_2 + nn_3)
                        v_yy = T.axis.spatial(T.int64(16), yy_0 * T.int64(16) + yy_1 * T.int64(16) + yy_2 * T.int64(16) + yy_3)
                        v_xx = T.axis.spatial(T.int64(16), xx_0 * T.int64(8) + xx_1 * T.int64(8) + xx_2 * T.int64(4) + xx_3)
                        v_ff = T.axis.spatial(T.int64(64), ff_0 * T.int64(64) + ff_1 * T.int64(8) + ff_2 * T.int64(2) + ff_3)
                        v_ry = T.axis.reduce(T.int64(7), ry_0 + ry_1)
                        v_rx = T.axis.reduce(T.int64(7), rx_0 * T.int64(7) + rx_1)
                        v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1)
                        T.reads(p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc], p1[v_ry, v_rx, v_rc, v_ff])
                        T.writes(conv2d_nhwc[v_nn, v_yy, v_xx, v_ff])
                        T.block_attr({"meta_schedule.tiling_structure": "SSRSRS"})
                        with T.init():
                            conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = T.float32(0)
                        conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] = conv2d_nhwc[v_nn, v_yy, v_xx, v_ff] + p0[v_nn, v_yy * T.int64(2) + v_ry, v_xx * T.int64(2) + v_rx, v_rc] * p1[v_ry, v_rx, v_rc, v_ff]
                for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(8), T.int64(64)):
                    with T.block("T_relu"):
                        v_ax0, v_ax1 = T.axis.remap("SS", [ax0, ax1])
                        v_ax2 = T.axis.spatial(T.int64(16), xx_0 * T.int64(8) + ax2)
                        v_ax3 = T.axis.spatial(T.int64(64), ax3)
                        T.reads(conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3], p2[v_ax0, T.int64(0), T.int64(0), v_ax3], p3[v_ax0, T.int64(0), T.int64(0), v_ax3], p4[v_ax0, T.int64(0), T.int64(0), v_ax3])
                        T.writes(T_relu[v_ax0, v_ax1, v_ax2, v_ax3])
                        T_relu[v_ax0, v_ax1, v_ax2, v_ax3] = T.max((conv2d_nhwc[v_ax0, v_ax1, v_ax2, v_ax3] + p2[v_ax0, T.int64(0), T.int64(0), v_ax3]) * p3[v_ax0, T.int64(0), T.int64(0), v_ax3] + p4[v_ax0, T.int64(0), T.int64(0), v_ax3], T.float32(0))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nhwc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l6, factors=[v13, v14, v15, v16], preserve_unit_iters=True, disable_predication=False)
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 16])
l25, l26, l27, l28 = sch.split(loop=l7, factors=[v21, v22, v23, v24], preserve_unit_iters=True, disable_predication=False)
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l33, l34, l35, l36 = sch.split(loop=l8, factors=[v29, v30, v31, v32], preserve_unit_iters=True, disable_predication=False)
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 8, 4, 2])
l41, l42, l43, l44 = sch.split(loop=l9, factors=[v37, v38, v39, v40], preserve_unit_iters=True, disable_predication=False)
v45, v46 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[7, 1])
l47, l48 = sch.split(loop=l10, factors=[v45, v46], preserve_unit_iters=True, disable_predication=False)
v49, v50 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 7])
l51, l52 = sch.split(loop=l11, factors=[v49, v50], preserve_unit_iters=True, disable_predication=False)
v53, v54 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l55, l56 = sch.split(loop=l12, factors=[v53, v54], preserve_unit_iters=True, disable_predication=False)
sch.reorder(l17, l25, l33, l41, l18, l26, l34, l42, l47, l51, l55, l19, l27, l35, l43, l48, l52, l56, l20, l28, l36, l44)
b57, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b57, loop=l41, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
2024-04-30 19:18:52 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 19:18:52 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2024-04-30 19:18:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 19:18:53 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2024-04-30 19:18:54 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 19:18:55 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 19:18:57 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 19:18:58 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 19:18:58 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9999  0.9998  0.9996  0.9989  0.9981  0.9973  0.9969  0.9967  0.9967  0.9967  0.9966  0.9965  0.9964  0.9963  0.9961  0.9960
[17 : 32]:	0.9953  0.9947  0.9944  0.9944  0.9942  0.9938  0.9937  0.9929  0.9927  0.9921  0.9917  0.9913  0.9910  0.9905  0.9898  0.9886
[33 : 48]:	0.9877  0.9870  0.9868  0.9865  0.9863  0.9863  0.9852  0.9846  0.9838  0.9819  0.9816  0.9813  0.9808  0.9808  0.9806  0.9804
[49 : 64]:	0.9804  0.9796  0.9796  0.9774  0.9773  0.9767  0.9764  0.9740  0.9736  0.9712  0.9710  0.9709  0.9708  0.9702  0.9700  0.9698
2024-04-30 19:18:58 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 19:18:58 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #1: GFLOPs: 28.6674. Time: 170.3133 us. Best GFLOPs: 28.6674
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #2: GFLOPs: 167.7532. Time: 29.1048 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #3: GFLOPs: 37.3949. Time: 130.5640 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #4: GFLOPs: 48.9524. Time: 99.7383 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #5: GFLOPs: 23.8208. Time: 204.9653 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #6: GFLOPs: 29.9828. Time: 162.8412 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #7: GFLOPs: 48.1526. Time: 101.3950 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #8: GFLOPs: 28.1846. Time: 173.2306 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #9: GFLOPs: 103.1641. Time: 47.3269 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #10: GFLOPs: 18.3097. Time: 266.6584 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #11: GFLOPs: 105.1615. Time: 46.4279 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #12: GFLOPs: 84.0261. Time: 58.1062 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #13: GFLOPs: 6.3315. Time: 771.1355 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #14: GFLOPs: 63.9615. Time: 76.3339 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #15: GFLOPs: 21.2767. Time: 229.4732 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #16: GFLOPs: 77.9726. Time: 62.6173 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #17: GFLOPs: 80.6463. Time: 60.5413 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #18: GFLOPs: 23.8502. Time: 204.7126 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #19: GFLOPs: 28.0905. Time: 173.8111 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #20: GFLOPs: 37.7731. Time: 129.2567 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #21: GFLOPs: 9.8440. Time: 495.9785 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #22: GFLOPs: 33.4334. Time: 146.0345 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #23: GFLOPs: 20.7059. Time: 235.7994 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #24: GFLOPs: 81.3746. Time: 59.9995 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #25: GFLOPs: 58.4372. Time: 83.5501 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #26: GFLOPs: 80.9371. Time: 60.3238 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #27: GFLOPs: 116.5222. Time: 41.9013 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #28: GFLOPs: 29.0892. Time: 167.8434 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #29: GFLOPs: 30.5699. Time: 159.7137 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #30: GFLOPs: 65.5956. Time: 74.4323 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #31: GFLOPs: 53.3822. Time: 91.4617 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #32: GFLOPs: 24.9331. Time: 195.8215 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #33: GFLOPs: 16.7006. Time: 292.3499 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #34: GFLOPs: 51.9156. Time: 94.0455 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #35: GFLOPs: 157.2800. Time: 31.0429 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #36: GFLOPs: 33.8402. Time: 144.2792 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #37: GFLOPs: 28.2698. Time: 172.7087 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #38: GFLOPs: 33.4883. Time: 145.7953 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #39: GFLOPs: 139.6924. Time: 34.9513 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #40: GFLOPs: 31.8265. Time: 153.4077 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #41: GFLOPs: 61.9187. Time: 78.8524 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #42: GFLOPs: 18.7808. Time: 259.9693 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #43: GFLOPs: 54.0872. Time: 90.2696 us. Best GFLOPs: 167.7532
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #44: GFLOPs: 174.8442. Time: 27.9245 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #45: GFLOPs: 60.0832. Time: 81.2612 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #46: GFLOPs: 31.5719. Time: 154.6450 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #47: GFLOPs: 140.9260. Time: 34.6454 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #48: GFLOPs: 14.0962. Time: 346.3646 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #49: GFLOPs: 34.2634. Time: 142.4969 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #50: GFLOPs: 24.5450. Time: 198.9177 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #51: GFLOPs: 12.4812. Time: 391.1814 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #52: GFLOPs: 8.6168. Time: 566.6208 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #53: GFLOPs: 19.1124. Time: 255.4591 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #54: GFLOPs: 22.6046. Time: 215.9925 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #55: GFLOPs: 27.7085. Time: 176.2072 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #56: GFLOPs: 21.9212. Time: 222.7263 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #57: GFLOPs: 19.2522. Time: 253.6040 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #58: GFLOPs: 82.2127. Time: 59.3878 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #59: GFLOPs: 111.9794. Time: 43.6012 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #60: GFLOPs: 90.5410. Time: 53.9251 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #61: GFLOPs: 19.2522. Time: 253.6032 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #62: GFLOPs: 12.5339. Time: 389.5377 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #63: GFLOPs: 72.8667. Time: 67.0050 us. Best GFLOPs: 174.8442
2024-04-30 19:36:26 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #64: GFLOPs: 12.6153. Time: 387.0240 us. Best GFLOPs: 174.8442
2024-04-30 20:15:30 [INFO] [evolutionary_search.cc:713] Generating candidates......
2024-04-30 20:15:30 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2024-04-30 20:15:31 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 20:15:31 [INFO] [evolutionary_search.cc:723] Sampled 448 candidate(s)
2024-04-30 20:15:35 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 20:15:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 20:15:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 20:15:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x93d9e58)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x647fb38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0xd51e618)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2e31e128)]: 0 failure(s)
2024-04-30 20:15:45 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9888  0.9655  0.9655  0.9579  0.9579  0.9486  0.9486  0.9486  0.9410  0.9410  0.9410  0.9277  0.9277  0.9276  0.9276  0.9138
[17 : 32]:	0.9137  0.9124  0.9124  0.9065  0.9065  0.9060  0.9060  0.8916  0.8728  0.8694  0.8675  0.8645  0.8645  0.8578  0.8563  0.8563
[33 : 48]:	0.8563  0.8563  0.8542  0.8542  0.8466  0.8459  0.8431  0.8429  0.8373  0.8370  0.8370  0.8370  0.8370  0.8370  0.8343  0.8343
[49 : 64]:	0.8343  0.8337  0.8335  0.8335  0.8334  0.8247  0.8225  0.8212  0.8196  0.8168  0.8143  0.8124  0.8113  0.8108  0.8075  0.8071
2024-04-30 20:15:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2024-04-30 20:15:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #65: GFLOPs: 134.2058. Time: 36.3802 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #66: GFLOPs: 137.2424. Time: 35.5752 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #67: GFLOPs: 142.9955. Time: 34.1440 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #68: GFLOPs: 150.6159. Time: 32.4164 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #69: GFLOPs: 160.0322. Time: 30.5091 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #70: GFLOPs: 139.8122. Time: 34.9214 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #71: GFLOPs: 147.3713. Time: 33.1301 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #72: GFLOPs: 114.8334. Time: 42.5175 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #73: GFLOPs: 152.4172. Time: 32.0333 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #74: GFLOPs: 149.3830. Time: 32.6840 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #75: GFLOPs: 146.5500. Time: 33.3158 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #76: GFLOPs: 143.0150. Time: 34.1393 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #77: GFLOPs: 149.3031. Time: 32.7015 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #78: GFLOPs: 138.3189. Time: 35.2984 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #79: GFLOPs: 135.6895. Time: 35.9824 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #80: GFLOPs: 141.4179. Time: 34.5249 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #81: GFLOPs: 108.6290. Time: 44.9459 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #82: GFLOPs: 159.9545. Time: 30.5239 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #83: GFLOPs: 152.3605. Time: 32.0453 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #84: GFLOPs: 115.3773. Time: 42.3171 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #85: GFLOPs: 113.6219. Time: 42.9709 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #86: GFLOPs: 93.7394. Time: 52.0851 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #87: GFLOPs: 109.9836. Time: 44.3924 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #88: GFLOPs: 143.0673. Time: 34.1268 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #89: GFLOPs: 165.6288. Time: 29.4782 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #90: GFLOPs: 97.4489. Time: 50.1025 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #91: GFLOPs: 107.6011. Time: 45.3753 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #92: GFLOPs: 117.7671. Time: 41.4584 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #93: GFLOPs: 123.2673. Time: 39.6085 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #94: GFLOPs: 79.7471. Time: 61.2239 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #95: GFLOPs: 95.6078. Time: 51.0673 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #96: GFLOPs: 152.8523. Time: 31.9422 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #97: GFLOPs: 152.0439. Time: 32.1120 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #98: GFLOPs: 146.3021. Time: 33.3723 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #99: GFLOPs: 159.1492. Time: 30.6783 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #100: GFLOPs: 163.0929. Time: 29.9365 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #101: GFLOPs: 154.2060. Time: 31.6617 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #102: GFLOPs: 91.2824. Time: 53.4871 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #103: GFLOPs: 126.3001. Time: 38.6574 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #104: GFLOPs: 167.4638. Time: 29.1551 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #105: GFLOPs: 103.0158. Time: 47.3950 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #106: GFLOPs: 144.8891. Time: 33.6977 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #107: GFLOPs: 74.2108. Time: 65.7914 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #108: GFLOPs: 126.7534. Time: 38.5191 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #109: GFLOPs: 133.0146. Time: 36.7060 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #110: GFLOPs: 141.0784. Time: 34.6079 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #111: GFLOPs: 77.4121. Time: 63.0706 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #112: GFLOPs: 79.7279. Time: 61.2387 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #113: GFLOPs: 68.3932. Time: 71.3877 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #114: GFLOPs: 70.6661. Time: 69.0916 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #115: GFLOPs: 123.4764. Time: 39.5414 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #116: GFLOPs: 117.1735. Time: 41.6684 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #117: GFLOPs: 118.6486. Time: 41.1504 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #118: GFLOPs: 125.9185. Time: 38.7745 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #119: GFLOPs: 119.5087. Time: 40.8542 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #120: GFLOPs: 76.6352. Time: 63.7100 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #121: GFLOPs: 94.9072. Time: 51.4443 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #122: GFLOPs: 172.8285. Time: 28.2502 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #123: GFLOPs: 170.7005. Time: 28.6023 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #124: GFLOPs: 100.9707. Time: 48.3550 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #125: GFLOPs: 58.7358. Time: 83.1254 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #126: GFLOPs: 72.3574. Time: 67.4766 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #127: GFLOPs: 63.1983. Time: 77.2558 us. Best GFLOPs: 174.8442
2024-04-30 20:16:04 [INFO] [task_scheduler.cc:131] [Task #9: fused_nn_conv2d_add_multiply_add_nn_relu] Trial #128: GFLOPs: 20.0637. Time: 243.3471 us. Best GFLOPs: 174.8442
